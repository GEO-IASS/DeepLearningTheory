\documentclass[12pt]{article}
\usepackage[margin=.5in]{geometry}
\usepackage{amsmath}    % need for subequations
\usepackage{amsthm}     % need for proof environments
\usepackage{amssymb}    % need for symbols
\usepackage{graphicx}    % need for symbols
\usepackage{color}      % use if color is used in text
\usepackage{caption}
\usepackage{subcaption}
\usepackage{moreverb}
\usepackage{hyperref}   % use for hypertext links, including those to external documents and URLs

\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\V}{\mathbb{V}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Laplace}{Laplace}
\DeclareMathOperator{\Gaussian}{Gaussian}
\DeclareMathOperator{\cV}{\mathcal{V}}
\DeclareMathOperator{\cF}{\mathcal{F}}
\DeclareMathOperator{\cH}{\mathcal{H}}
\DeclareMathOperator{\cX}{\mathcal{X}}
\DeclareMathOperator{\cY}{\mathcal{Y}}
\DeclareMathOperator{\cP}{\mathcal{P}}
\DeclareMathOperator{\cN}{\mathcal{N}}
\DeclareMathOperator{\VC}{VC}
\DeclareMathOperator{\KL}{KL}
\DeclareMathOperator{\ML}{ML}
\DeclareMathOperator{\EM}{EM}
\DeclareMathOperator{\MAP}{MAP}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\LOOCV}{LOOCV}
\DeclareMathOperator{\SV}{SV}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\D}{\mathcal{D}}
\DeclareMathOperator{\N}{\mathcal{N}}
\DeclareMathOperator{\HH}{\mathcal{H}}
\DeclareMathOperator{\PP}{\mathbb{P}}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Regret}{Regret}
\DeclareMathOperator{\bint}{\mathbf{int}}
\DeclareMathOperator{\dist}{\mathbf{dist}}
\renewcommand{\P}{\PP}
\renewcommand{\H}{\HH}
\DeclareMathOperator{\out}{out}
\DeclareMathOperator{\conv}{conv}
\DeclareMathOperator{\incomp}{in}
\DeclareMathOperator{\num}{num}
\DeclareMathOperator{\bernoulli}{Bernoulli}
\newtheorem{lemma}{Lemma} %[section] %[chapter]
\begin{document}
We considered a linearization of a neural net. That is, we defined nonlinearity
$F$ to be the identity function. Then the function $h_{W,b}(x) =
W^{(2)}(W^{(1)}x + b^{(1)}) + b^{(2)}$. We can rewrite this form as $h_{W,b}(x)
= W^{(2)}W^{(1)} x + b$. To simplify the problem, we constrain $W^{(2)} =
(W^{(1)})^T$ and $b = 0$. Let $W = W^{(1)}$. Then $h_{W,b}(x) = W^TW x$. We also
add a $\ell^2$ sparsity constraint $\|\rho - \hat{\rho}\|_2^2$. The minimization problem then becomes

\[\min_W \sum_{i=1}^m \|W^TWx^{(i)} - x^{(i)}\|_2^2  + \beta \|\rho - \hat{\rho}\|_2^2 \]

We explicitly derived the gradient of the above formula and implemented gradient
descent.
\end{document}
