% small.tex
\documentclass{beamer}
\usepackage{dcolumn}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\ra}{\rightarrow}
\newcommand{\Wo}{W^{(1)}}
\newcommand{\Wt}{W^{(2)}}
\newcommand{\bo}{b^{(1)}}
\newcommand{\bt}{b^{(2)}}
\newcommand{\zr}{z^{(3)}}
\newcommand{\zt}{z^{(2)}}
\newcommand{\ar}{a^{(3)}}
\newcommand{\at}{a^{(2)}}
\newcommand{\ao}{a^{(1)}}
\newcommand{\dr}{\delta^{(3)}}
\newcommand{\dt}{\delta^{(2)}}
\newcommand{\xii}{x^{(i)}}
\DeclareMathOperator{\KL}{KL}
\newcommand{\pd}[2]{\ensuremath{\cfrac{\partial #1}{\partial #2}}}

\title[]{Effects of Sparsity and the Activation Function on Sparse Autoencoders}
\author[]{Justin Johnson\quad Bharath Ramsundar}
\date{}
\usetheme{default}
\begin{document}
\begin{frame}
\maketitle
\end{frame}

\begin{frame}{Sparse Autoencoders}

Autoencoders are neural networks that try to learn the identity function.
Sparsity constraints force the learned representation to be nontrivial.

\begin{figure}[htb]
\begin{center}
\label{proc_sched}
\includegraphics[width=0.5\textwidth]{Autoencoder636.png}
\caption{Autoencoder}
\end{center}
\end{figure}

\end{frame}
\begin{frame}{Cost Function}
\small
Let the hidden layer have $p$ units, and let $f:\RR\ra\RR$ be a differentiable
activation function. The neural network is parameterized by terms weights
$\Wo\in\RR^{p\times n}$ and $\Wt\in\RR^{n\times p}$ and bias terms $\bo\in\RR^p$
and $\bt\in\RR^n$. The prediction on an input
$x\in\RR^n$ is
\[h_{W,b}=f(\Wt f(\Wo x+\bo)+\bt)\]
Given training examples $x^{(1)},\ldots,x^{(m)}\in\RR^n$ the objective function is
\[J(W,b)=\frac1m\sum_{i=1}^m\ell(h_{W,b}(\xii),\xii)+\lambda\psi(W,b)+\beta\sum_{j=1}^p\phi(\hat\rho_j)\]
where 
\[\hat\rho_j=\frac1m\sum_{j=1}^mf\left((\Wo_j)^T\xii+\bo_j\right)\] is 
the average activation of the $j$th hidden unit and
$\ell:\RR^n\times\RR^n\ra\RR$ is a loss function. The function $\psi$ is a
regularizer, and $\phi$ is the sparsity function.
\normalsize
\end{frame}
\footnotesize
\begin{frame}{Pictures}
\begin{table}[h!]
\centering
\begin{tabular}{ | c | m{2cm} | m{2cm} | m{2cm} | m{2cm}| }
\hline
Activation & None & $L^1$ & $L^2$ & $\KL$ \\ \hline
Sigmoid
&
% VERIFIED
\begin{minipage}{.3\textwidth}
\includegraphics[width=20mm, height=20mm]{fig3}
\end{minipage}
\tiny
Train: $94.45\%$, Test: $82.8\%$ 
& 
% VERIFIED
\begin{minipage}{.3\textwidth}
\includegraphics[width=20mm, height=20mm]{fig1}
\end{minipage}
\tiny
Train: $93.9\%$, Test: $83.2\%$ 
& 
% VERIFIED
\begin{minipage}{.3\textwidth}
\includegraphics[width=20mm, height=20mm]{fig8}
\end{minipage}
\tiny
Train: $94.65\%$, Test: $82.85\%$ 
& 
% VERIFIED
\begin{minipage}{.3\textwidth}
\includegraphics[width=20mm, height=20mm]{fig2}
\end{minipage}
\tiny
Train: $94.15\%$, Test: $83.55\%$ 
\\ \hline
Identity
&
% VERIFIED
\begin{minipage}{.3\textwidth}
\includegraphics[width=20mm, height=20mm]{fig6}
\end{minipage}
\tiny
Train: $99.65\%$, Test: $80.1\%$ 
& 
% VERIFIED
\begin{minipage}{.3\textwidth}
\includegraphics[width=20mm, height=20mm]{fig5}
\end{minipage}
\tiny
Train: $99.7\%$, Test: $79.6\%$ 
& 
%\begin{minipage}{.3\textwidth}
%\includegraphics[width=20mm, height=20mm]{fig5}
%\end{minipage}
%\tiny
%Train: $23$, Test: $23$ 
& 
%\begin{minipage}{.3\textwidth}
%\includegraphics[width=20mm, height=20mm]{fig6}
%\end{minipage}
%\tiny
%Train: $23$, Test: $23$ 
\\ \hline
Sine
&
% VERIFIED
\begin{minipage}{.3\textwidth}
\includegraphics[width=20mm, height=20mm]{fig7}
\end{minipage}
\tiny
Train: $99.7\%$, Test: $81.95\%$ 
& 
%\begin{minipage}{.3\textwidth}
%\includegraphics[width=20mm, height=20mm]{fig5}
%\end{minipage}
%\tiny
%Train: $23$, Test: $23$ 
& 
%\begin{minipage}{.3\textwidth}
%\includegraphics[width=20mm, height=20mm]{fig5}
%\end{minipage}
%\tiny
%Train: $23$, Test: $23$ 
& 
%\begin{minipage}{.3\textwidth}
%\includegraphics[width=20mm, height=20mm]{fig6}
%\end{minipage}
%\tiny
%Train: $23$, Test: $23$ 
\\ \hline
\end{tabular}
%\caption{my.Lboro Analysis}\label{tbl:myLboro}
\end{table}
\end{frame}

\end{document}
