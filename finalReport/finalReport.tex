\documentclass[twocolumn]{article}
\usepackage{array}
\usepackage{fullpage}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{url}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{footnote}
\usepackage[margin=1.9cm]{geometry}

\makesavenoteenv{tabular}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\ra}{\rightarrow}
\newcommand{\Wo}{W^{(1)}}
\newcommand{\Wt}{W^{(2)}}
\newcommand{\bo}{b^{(1)}}
\newcommand{\bt}{b^{(2)}}
\newcommand{\zr}{z^{(3)}}
\newcommand{\zt}{z^{(2)}}
\newcommand{\ar}{a^{(3)}}
\newcommand{\at}{a^{(2)}}
\newcommand{\ao}{a^{(1)}}
\newcommand{\dr}{\delta^{(3)}}
\newcommand{\dt}{\delta^{(2)}}
\newcommand{\xii}{x^{(i)}}
\newcommand{\pd}[2]{\ensuremath{\cfrac{\partial #1}{\partial #2}}}

\title{Properties of Autoencoders}
\author{%\sectionsize
    Justin Johnson \\
    \texttt{jcjohns@stanford.edu}
  \and
    Bharath Ramsundar \\
    \texttt{rbharath@stanford.edu}
}
\date{}

\begin{document}

\maketitle

\section{Introduction}
In recent years a variety of deep learning algorithms (including deep belief
networks \cite{hinton2006fast,lee2009convolutional} and deep neural networks,
\cite{krizhevsky2012imagenet}, \cite{le2011building}) have risen to prominence.
The types of networks that are typically used in these applications are 
complicated, and studying their theoretical properties is difficult. In this
project, we take first steps towards greater theoretical understanding of
deep learning by surveying the prior literature and performing empirical
studies.

We focus our work on a simple building-block of complicated deep networks: the
sparse autoencoders. Past approaches have utilized features learned from
unsupervised pretraining of deep neural networks in order to improve performance
on classification tasks \cite{le2011building}. The sparse autoencoder is a
fundamental building block of this unsupervised pretraining process. A
single-layer autoencoder is a much simpler object than an entire deep network,
but even this relatively simple object has not been well-studied theoretically.
In this project we aim to explore different varieties of autoencoders and
understand why they work.

\section{Sparse Autoencoder} A sparse autoencoder is a neural network with a
single hidden layer. Autoencoders attempts to learn nontrivial representations
of the identity function on the training example. These representations are
consequently used as input features for classifiers. 

Autoencoders are parameterized by choice of transfer function $f$, choice of
sparsity $\phi$, and choice of regularization $\psi$.  Neural net weights $W,b$
are learned by minimizing an objective function $J$ dependent on these choices:
\[J(W,b)=\frac1m\sum_{i=1}^m\ell(h_{W,b}(\xii),\xii)+\lambda\psi(W,b)+\beta\sum_{j=1}^p\phi(\hat\rho_j)\]
The first term is the $\ell^2$ reconstruction error of the training
data. The second term is $\psi$-regularization of the weight vectors, and the
third term enforces a $\phi$-sparsity constraint on the mean activation of each hidden layer neuron over the training set. Further details are given in Appendix
\ref{gen_sparse_auto}.

\section{Theoretical Underpinnings}
In this section we survey certain theoretical properties of neural nets and
autoencoders. We start with a general property of neural nets, the universal
approximation result \cite{cybenko1989approximation}, which proves that
arbitrary continuous functions may be uniformly approximated by single-layer
nets. We continue to discuss a NP-Hardness result \cite{blum1992training}, which
establishes the complexity of training even a single-layer net. We end by
discussing a surprising result which shows that, in the absence of sparsity and
regularization, autoencoders are simply a form of PCA \cite{bourlard1988auto}.

\subsection{Universal Approximation}
Neural networks with one hidden layer are capable of uniformly approximating
arbitrary continuous functions \cite{cybenko1989approximation}. More precisely,
finite linear combinations of the form
\[\sum_{j=1}^N \alpha_j \sigma(w_j^T x + b_j), \]
(where $\sigma$ is a ``sigmoidal'' function, \textit{i.e.}, $\lim_{x \to \infty}
\sigma(x) = 1$ and $\lim_{x \to -\infty} \sigma(x) = 0$) can uniformly
approximate any continuous function on $\R^n$.  This result follows from basic
functional analysis and from injectivity of the Fourier transform
\cite{rudin1991functional}. Further arguments based on the Stone-Weierstrass
theorem or Wiener's Tauberian theorem allow the result to be extended to more
general classes of activation functions such as $\sigma = \sin(nt)$ or $\sigma =
\exp(nt)$ \cite{rudin1991functional}.

Note that these approximation results don't apply directly to autoencoders; 
autoencoders are distinct from general neural networks, since
they attempt only to learn the identity. However, the universal
approximation result motivates the belief that autencoders can learn
highly nontrivial, data-dependent representations of the identity.

\subsection{NP-Hardness}
Training a neural network classifier with $n$-dimensional inputs and only $3$
hidden units is NP-hard when $\sigma$ is a Heaviside step function
\cite{blum1992training}. The proof shows that there exist some sets of training
data $X$ such that learning a correct classifier on the data is combinatorially
intractable. This result emphasizes the fact that training of a
general neural network may be infeasible. However, the hardness result holds
only for a specific class of network, and as the next section will show, certain
types of neural networks may be efficiently learned.

\subsection{Equivalence with PCA}
An autoencoder that does not include regularization $\psi$ or sparsity $\phi$
learns a feature representation that is closely related to the principal
components of the training data \cite{bourlard1988auto}. In the absence of
$\phi$ or $\psi$, the cost vector is simply the $L^2$ difference between the
training data and autoencoder output. Using the fact that singular value
decomposition provides the optimal rank $p$ approximation to a $n$ dimensional
vector, we may then directly solve for the weight and bias vectors.

The interesting consequence is that the nonlinearity of the autoencoder,
dependent on choice of activation $f$, is meaningless without appropriate choice
of regularizer or sparsity. This result motivates our in-depth empirical study
of the relations between the activation and sparsity functions in the next
section.

% \section{Sparse Linear Autoencoders}
% We next considered a sparse linearized autoencoder where the transfer function
% is simply the identity function. The objective function for this autoencoder is
% the sum of the $\ell^2$ reconstruction error, an $\ell^2$ regularization term on
% the network weights, and an $\ell^2$ sparsity constraint of the form
% $\|\rho-\hat\rho\|_2^2$ where as above $\hat\rho_j$ is the mean activation of
% the $j$th hidden unit on the training set. The learned features for this
% autoencoder is shown in Figure~\ref{fig:features}.

\section{Experiments}
\subsection{Equivalence with PCA}
We first studied the empirical relationship between PCA and autoencoders with
various activation functions. In order to remain consistent with the analysis
in \cite{bourlard1988auto} we set $\lambda=\beta=0$ to omit the regularization
and sparsity penalty terms. For each choice of activation function we trained
an autoencoder using 2000 images from the MNIST \cite{lecun1998mnist} training
set.

Once an autoencoder has been trained, we can treat the activations of the hidden
units on any input as a feature transform of that input. In the language of
Appendix~\ref{gen_sparse_auto}, this feature transform is $\at(x)=f(\Wo x+\bo)$.
We can visualize this feature transform by viewing each row of $\Wo$ as a linear
filter against which the input $x$ is matched.

For each activation function considered, the feature transform of the corresponding
trained autoencoder was used to train a softmax classifier
to recognize digits. Each classifier was evaluated on both the training set and
a test set consisting of 2000 additional MNIST images.

We also computed the first 200 principal components of the training data.
Projecting any input $x$ onto the first 200 principal components of the
training data gives a feature transform of the input $x$. We used this feature
transform to train a softmax classifier.

Table~\ref{table:noSparse} shows a subset of the features learned by each
autoencoder, a subset of the first 200 principal components of the data, and
the performance of each trained softmax classifier on the training and testing
sets. Qualitatively, some of the features learned by the autoencoders look
similar to the principal components of the training data. The sigmoid, identity,
and sinusoidal activation functions give rise to feature transforms that perform
very similarly to the PCA feature transform. The feature transform learned from
the arctangent activation function appears to be underfitting the training data;
this could indicate that the underlying autoencoder was not fully trained.

\subsection{Generalized Sparse Autoencoders}
The theoretical results of \cite{bourlard1988auto} and the empirical results of
the previous section suggest that autoencoders cannot learn novel feature
transforms unless the optimization objective is modified. To this end, we
trained autoencoders using a variety of activation functions and sparsity
constraint functions. We use the regularizer
$\psi(W,b)=\frac12\|\Wo\|_F+\frac12\|\Wt\|_F$
where $\|A\|_F$ is the Frobenius norm.

As in the previous section, the feature transform of each trained autoencoder
is used to train a softmax classifier. The results of these experiments are
found in Table~\ref{table:sparse-ae}. In all cases we take $\rho=0.1$.

The autoencoders trained with the empty sparsity penalty $\phi(\hat\rho_j)=0$
are essentially the same as the autoencoders from the previous section with the
addition of regularization on $W$. The learned features are similar, but the
softmax classifier enjoys lower generalization error.

The $\ell^1$ sparsity penalty $\phi(\hat\rho_j)=|\hat\rho_j-\rho|$ gives rise
to classifiers that perform worse than a softmax classifier trained only on the
pixel values of the training set. We did not experiment extensively with different
values of the parameters $\lambda,\beta$, and $\rho$; it is possible that a better
choice of parameters would result in better classifiers in these cases.

For all but the identity activation function, 
the $\ell^2$ sparsity penalty $\phi(\hat\rho_j)=(\hat\rho_j-\rho)^2$ and the KL
sparsity penalty give rise to classifiers that significantly outperform all other
classifiers considered.

\section{Discussion}
Our experimentation so far has led us to focus on the sparsity constraint of the
autoencoder.  For the remainder of the term we will consider other variants on
this constraint both on nonlinear and linearized networks. In particular, we
will experiment with $\ell^1$ and $\ell^2$ sparsity constraints on the nonlinear
network and an $\ell^1$ sparsity constraint on the linearized network. Depending
on the results of these experiments, we will attempt to understand the
theoretical implications of these sorts of constraints. In addition to the
linearized autoencoder, we would also like to experiment with networks whose
transfer function is a Taylor approximation to the sigmoid function.

\newcolumntype{C}{ >{\centering\arraybackslash} m{0.155\linewidth} }
\newcolumntype{D}{ >{\centering\arraybackslash} m{0.145\linewidth} }
% \begin{table*}
%   \centering
%   \begin{tabular}{CCCC}
%     Transfer function $f(z)$ & Learned Features & Training Error & Testing Error \\
%     \hline
%     $1/(1+e^{-z})$ & \includegraphics[width=0.15\textwidth]{aeFigs/sigmoid.png} & 99.95\%  & 82.00\% \\
%     $\tan^{-1}(z)$ & \includegraphics[width=0.15\textwidth]{aeFigs/atan.png} & 90.50\% & 80.05\% \\
%     $z$ & \includegraphics[width=0.15\textwidth]{aeFigs/id.png} & 99.95\% & 82.00\% \\
%     $\sin(z)$ & \includegraphics[width=0.15\textwidth]{aeFigs/sin.png} & 100.00\% & 81.80\% \\
%     \hline
%     PCA & \includegraphics[width=0.15\textwidth]{aeFigs/pca.png} & 100.00\% & 82.30\% \\
%   \end{tabular}
% \end{table*}

\begin{table*}
  \centering
  \begin{tabular}{|C|DDDD|D|}
  \hline
  Activation function $f(z)$ & $1/(1+e^{-z})$ & $1/2+\tan^{-1}(z)/\pi$ & $z$ & $1/2+\sin(z)/2$ & PCA \\
  Learned features
    & \includegraphics[width=0.15\textwidth]{aeFigs/sigmoid.png}
    & \includegraphics[width=0.15\textwidth]{aeFigs/atan.png}
    & \includegraphics[width=0.15\textwidth]{aeFigs/id.png}
    & \includegraphics[width=0.15\textwidth]{aeFigs/sin.png}
    & \includegraphics[width=0.15\textwidth]{aeFigs/pca.png} \\
  Training accuracy & 99.95\% & 90.50\% & 99.95\% & 99.95\% & 100.00\% \\
  Testing accuracy & 82.00\% & 80.05\% & 82.00\% & 79.60\% & 82.30\% \\
  \hline
  \end{tabular}
  \caption{Single layer autoencoders with no regularization or sparsity penalty were trained on
    2000 MNIST images and the extracted feature transform was used to train a softmax classifier.
    In all cases we learned a 200-dimensional feature transform; a small subset of these features
    are shown. We compare with a softmax classifier trained on PCA features extracted from the same
    training data. In all cases the trained feature transform and classifier are tested on a different
    set of 2000 MNIST images. For reference, a softmax classifier trained directly from the pixel values
    of the training data achieves a training accuracy of 100.00\% and a testing accuracy of 82.90\%.}
  \label{table:noSparse}
\end{table*}

\newcommand{\figalign}[3]{
  \begin{minipage}[c]{\linewidth}
    \centering
    \vspace{0.25pc}
    \includegraphics[width=\linewidth]{#1} \\*
    #2 \hspace*{0pc} \\*
    #3
    \vspace{0.25pc}
  \end{minipage}
}
\begin{savenotes} 
\begin{table*}
  \centering
  \begin{tabular}{|c|C|CCCC|}
    \hline
    && \multicolumn{4}{c|}{Activation function $f(z)$} \\ \cline{3-6}
    && $1/(1+e^{-z})$ & $1/2+\tan^{-1}(z)/\pi$ & $z$ & $1/2+\sin(z)/2$ \\ \hline
    \multirow{4}{*}{\begin{sideways}\hspace{-4pc}Sparsity penalty $\phi(\hat\rho_j)$\end{sideways}}    
    & 0 
      & \figalign{saeFigs/sigmoidNone.png}{98.75\%}{84.90\%}
      & \figalign{saeFigs/atanNone.png}{98.90\%}{85.20\%}
      & \figalign{saeFigs/idNone.png}{100.00\%}{82.85\%}
      & \figalign{saeFigs/sineNone.png}{99.90\%}{83.00\%} \\ \cline{2-6}
    & $|\hat\rho_j-\rho|$
      & \figalign{saeFigs/sigmoidL1.png}{88.50\%}{80.45\%}
      & \figalign{saeFigs/atanL1.png}{75.55\%}{67.45\%}
      & \figalign{saeFigs/idL1.png}{99.60\%}{79.85\%}
      & \figalign{saeFigs/sineL1.png}{96.30\%}{79.90\%} \\ \cline{2-6}
    & $(\hat\rho_j-\rho)^2$
      & \figalign{saeFigs/sigmoidL2.png}{99.85\%}{91.30\%}
      & \figalign{saeFigs/atanL2.png}{99.75\%}{90.45\%}
      & \figalign{saeFigs/idL2.png}{100.00\%}{83.30\%}
      & \figalign{saeFigs/sineL2.png}{99.85\%}{88.45\%} \\ \cline{2-6}
    & $KL(\rho\|\hat\rho_j)$
      & \figalign{saeFigs/sigmoidKL.png}{99.75\%}{90.30\%}
      & \figalign{saeFigs/atanKL.png}{99.50\%}{90.00\%}
      & \figalign{saeFigs/X.png}{}{}\footnote{The KL sparsity penalty is only defined when the activations are constrained to lie in
        $(0,1)$.}
        & \figalign{saeFigs/sineKL.png}{99.70\%}{88.90\%} \\
    \hline
  \end{tabular}
  \caption{We trained single layer autoencoders with $\ell^2$ regularization
    and a variety of activation functions and sparsity constraints. In all cases
    we learned a 200-dimensional feature transform; for each case we show a
    small representative subset of these features. Each autoencoder was trained
    using 2000 MNIST images and in each case the extracted feature transform was
    used to train a softmax classifier. The final classifier was then tested on
    a different set of 2000 MNIST images. The accuracy on the training and test
    sets respectively are shown beneath each visualized feature transform.}
  \label{table:sparse-ae}
\end{table*}
\end{savenotes}

% \begin{figure*}
%   \centering
%   \includegraphics[width=0.45\textwidth]{sparsesigmoidnn.png}
%   \includegraphics[width=0.45\textwidth]{linearnn.png}
%   \includegraphics[width=0.45\textwidth]{pca.png}
%   \includegraphics[width=0.45\textwidth]{nonlinearnn.png}
%   \caption{Learned features for MNIST using different methods.
%       Upper left: Sparse (nonlinear) autoencoder.
%       Upper right: Sparse linear autoencoder.
%       Lower left: Principal component analysis.
%       Lower right: nonlinear autoencoder.
%     }
%   \label{fig:features}
% \end{figure*}

\appendix
\section{Generalized Sparse Autoencoders}
\label{gen_sparse_auto}
In this section we describe a general framework for sparse autoencoders.
This is a generalization of the sparse autoencoder presented in \cite{ufldl-tutorial}.
An autoencoder is a neural network with a single hidden layer that attempts to learn the identity function
$\RR^n\ra\RR^n$. Let the hidden layer have $p$ units, and let $f:\RR\ra\RR$ be a differentiable activation
function. The neural network is parameterized by terms weights $\Wo\in\RR^{p\times n}$ and
$\Wt\in\RR^{n\times p}$ and bias terms $\bo\in\RR^p$ and $\bt\in\RR^n$. Given weight and bias terms, the
prediction on an input $x\in\RR^n$ is
\[h_{W,b}=f(\Wt f(\Wo x+\bo)+\bt)\]
Given training examples $x^{(1)},\ldots,x^{(m)}\in\RR^n$ the objective function for the generalized sparse
autoencoder is
\[J(W,b)=\frac1m\sum_{i=1}^m\ell(h_{W,b}(\xii),\xii)+\lambda\psi(W,b)+\beta\sum_{j=1}^p\phi(\hat\rho_j)\]
where we define
\[\hat\rho_j=\frac1m\sum_{j=1}^mf\left((\Wo_j)^T\xii+\bo_j\right)\]
to be the average activation of the $j$th hidden unit over the training set; here $(\Wo_j)^T$ is the
$j$th row of the matrix $\Wo$. In the sparse autoencoder objective function, $\ell:\RR^n\times\RR^n\ra\RR$
is a loss function that encourages the autoencoder to have low reconstruction error.
The function $\psi$ is a regularizer for the weight
and bias terms and the parameter $\lambda\in\RR$ controls the relative importance of the regularization.
Frequently we define $\psi(W,b)=\frac12\|\Wo\|^2_F+\frac12\|\Wt\|^2_F$ where $\|A\|_F$ is the Frobenius norm.
The function $\phi:\RR\ra\RR$ is the sparsity penalty, and the parameter $\beta\in\RR$ controls the relative
importance of the sparsity penalty.

We train a generalized sparse autoencoder using gradient descent.
For the remainder of the discussion we assume that $\ell(x,y)=\frac12\|x-y\|^2$;
this allows us to use the standard backpropogation algorithm to compute
the gradient of the reconstruction error term. For notational convenience, for $x\in\RR^n$ define
\begin{align*}
  \zt(x) &=\Wo x+\bo && \at(x) = f(\zt(x)) \\
  \zr(x) &=\Wt\at(x)+\bt && \ar(x) = f(\zr(x))
\end{align*}
In the standard backpropogation algorithm we define
\begin{align*}
  \dr(x) &= -(x-\ar(x))\odot f'(\zr(x)) \\
  \dt(x) &= \left((\Wt)^T\dr(x)\right)\odot f'(\zt(x))
\end{align*}
where $\odot$ is componentwise multiplication of vectors and $f'$ is applied componentwise.
Then the gradients of the reconstruction terms are given by
\begin{align*}
  \nabla_{W^{(k)}}\ell(h_{W,b}(x),x) &= \delta^{(k+1)}(x)a^{(k)}(x)^T \\
  \nabla_{b^{(k)}}\ell(h_{W,b)}(x),x) &= \delta^{(k+1)}(x)
\end{align*}
where $k\in\{1,2\}$ and we define $\ao(x)=x$.

Computing the gradient of $\psi(W,b)$ is typically straightforward; for example when
\[\psi(W,b)=\frac12\|\Wo\|_F^2+\frac12\|\Wt\|_F^2\] then $\nabla_{W^{(k)}}\psi(W,b)=W^{(k)}$ and
$\nabla_{b^{(k)}}\psi(W,b)=0$ for $k\in\{1,2\}$.

It remains to compute the gradient for the sparsity term.
Clearly $\nabla\phi(\hat\rho_j)=\phi'(\hat\rho_j)\nabla\hat\rho_j$ and $\nabla_{\bt}\hat\rho_j=0$ and
$\nabla_{\Wt}\hat\rho_j=0$ so we only need to compute $\nabla_{\bo}\hat\rho_j$ and $\nabla_{\Wo}\hat\rho_j$.
A bit of arithmetic shows that
\begin{align*}
  \pd{\hat\rho_j}{\bo_k} &= \begin{cases} 
      \frac1m\sum_{i=1}^mf'\left((\Wo_j)^T\xii+\bo_j\right) & j=k \\
      0 & \textrm{otherwise}
  \end{cases} \\
    \nabla_{\Wo_k}\hat\rho_j &= \begin{cases}
      \frac1m\sum_{i=1}^mf'\left((\Wo_j)^T\xii+\bo_j\right)\xii & j=k \\
      0 & \textrm{otherwise}
    \end{cases}
\end{align*}
A bit more arithmetic shows that if we modify the standard backpropogation algorithm and by setting
\[\dt_i(x) = \left(\sum_{j=1}^p\Wt_{ji}\dr_j(x)+\beta\phi'(\hat\rho_i)\right)f'\left(\zt_i(x)\right)\]
the the gradient of the sparse autoencoder objective function is simply
\begin{align*}
  \nabla_{b^{(k)}}J(W,b) &= \frac1m\sum_{i=1}^m\delta^{(k+1)}(\xii) + \lambda\nabla_{b^{(k)}}\psi(W,b) \\
  \nabla_{W^{(k)}}J(W,b) &= \frac1m\sum_{i=1}^m\delta^{(k+1)}(\xii)a^{(k)}(\xii)^T
    + \lambda\nabla_{W^{(k)}}\psi(W,b)
\end{align*}

\bibliographystyle{plain}
\bibliography{refs}


\end{document}
