\relax 
\citation{hinton2006fast}
\citation{lee2009convolutional}
\citation{krizhevsky2012imagenet}
\citation{le2011building}
\citation{le2011building}
\citation{cybenko1989approximation}
\citation{blum1992training}
\citation{bourlard1988auto}
\citation{cybenko1989approximation}
\citation{rudin1991functional}
\citation{rudin1991functional}
\citation{blum1992training}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Sparse Autoencoder}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Theoretical Underpinnings}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Universal Approximation}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}NP-Hardness}{1}}
\citation{bourlard1988auto}
\citation{bourlard1988auto}
\citation{lecun1998mnist}
\citation{bourlard1988auto}
\citation{rumelhart2002learning}
\citation{lee2007efficient}
\citation{shen2008sparse}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Equivalence with PCA}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Equivalence with PCA}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Generalized Sparse Autoencoders}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{2}}
\citation{ufldl-tutorial}
\citation{liu1989limited}
\bibstyle{plain}
\bibdata{refs}
\bibcite{blum1992training}{1}
\bibcite{bourlard1988auto}{2}
\bibcite{cybenko1989approximation}{3}
\bibcite{hinton2006fast}{4}
\bibcite{krizhevsky2012imagenet}{5}
\@writefile{toc}{\contentsline {section}{\numberline {A}Generalized Sparse Autoencoders}{3}}
\newlabel{gen_sparse_auto}{{A}{3}}
\bibcite{le2011building}{6}
\bibcite{lecun1998mnist}{7}
\bibcite{lee2007efficient}{8}
\bibcite{lee2009convolutional}{9}
\bibcite{liu1989limited}{10}
\bibcite{ufldl-tutorial}{11}
\bibcite{rudin1991functional}{12}
\bibcite{rumelhart2002learning}{13}
\bibcite{shen2008sparse}{14}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Single layer autoencoders with no regularization or sparsity penalty were trained on 2000 MNIST images and the extracted feature transform was used to train a softmax classifier. In all cases we learned a 200-dimensional feature transform; a small subset of these features are shown. We compare with a softmax classifier trained on PCA features extracted from the same training data. In all cases the trained feature transform and classifier are tested on a different set of 2000 MNIST images. For reference, a softmax classifier trained directly from the pixel values of the training data achieves a training accuracy of 100.00\% and a testing accuracy of 82.90\%.}}{4}}
\newlabel{table:noSparse}{{1}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces We trained single layer autoencoders with $\ell ^2$ regularization and a variety of activation functions and sparsity constraints. In all cases we learned a 200-dimensional feature transform; for each case we show a small representative subset of these features. Each autoencoder was trained using 2000 MNIST images and in each case the extracted feature transform was used to train a softmax classifier. The final classifier was then tested on a different set of 2000 MNIST images. The accuracy on the training and test sets respectively are shown beneath each visualized feature transform. Note that the KL sparsity penalty is only defined when $\mathaccentV {hat}05E\rho _j\in (0,1)$; this is not true in general for $f(z)=z$.}}{5}}
\newlabel{table:sparse-ae}{{2}{5}}
