\relax 
\citation{hinton2006fast}
\citation{lee2009convolutional}
\citation{krizhevsky2012imagenet}
\citation{le2011building}
\citation{le2011building}
\citation{lecun1998mnist}
\citation{bourlard1988auto}
\citation{cybenko1989approximation}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Sparse Autoencoder}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Equivalence with PCA}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Universal Approximation}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Sparse Linear Autoencoders}{1}}
\citation{ufldl-tutorial}
\bibstyle{plain}
\bibdata{refs}
\bibcite{bourlard1988auto}{1}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Generalized Sparse Autoencoders}{2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Single layer autoencoders with no regularization or sparsity penalty were trained on 2000 MNIST images and the extracted feature transform was used to train a softmax classifier. In all cases we learned a 200-dimensional feature transform; a small subset of these features are shown. We compare with a softmax classifier trained on PCA features extracted from the same training data. In all cases the trained feature transform and classifier are tested on a different set of 2000 MNIST images.}}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Learned features for MNIST using different methods. Upper left: Sparse (nonlinear) autoencoder. Upper right: Sparse linear autoencoder. Lower left: Principal component analysis. Lower right: nonlinear autoencoder. }}{4}}
\newlabel{fig:features}{{1}{4}}
\bibcite{cybenko1989approximation}{2}
\bibcite{hinton2006fast}{3}
\bibcite{krizhevsky2012imagenet}{4}
\bibcite{le2011building}{5}
\bibcite{lecun1998mnist}{6}
\bibcite{lee2009convolutional}{7}
\bibcite{ufldl-tutorial}{8}
