\documentclass{article} % For LaTeX2e
\usepackage{nips12submit_e,times}
%\documentstyle[nips12submit_09,times,art10]{article} % For LaTeX 2.09


\title{A Survey of Theoretical Results on Deep Learning}

\author{
  Justin Johnson \\
  Department of Computer Science \\
  Stanford University \\
  \texttt{jcjohns@stanford.edu}
  \And
  Bharath Ramsundar \\
  Department of Computer Science \\
  Stanford University \\
  \texttt{rbharath@stanford.edu}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
  Recent results have shown that deep learning methods can be used to achieve state of the art results on many classification and regression problems \cite{krizhevsky2012imagenet}. However much of the contemporary literature has been empirical in nature; there has been comparatively less theoretical exploration. In this project we will survey the current theoretical understanding of deep learning methods.

A preliminary review of the literature has shown that some current results include universal approximation theorems for Boolean Deep Belief Nets \cite{le2010deep,sutskever2008deep} and an NP-completeness result for Boolean autoencoders\cite{baldi2012autoencoders}. There are also a number of classical results describing the universal approximation properties of shallow neural networks over various real function spaces\cite{siegelmann1995computational}.

One possible avenue for understanding these results is to empirically compare the convergence properties of deep belief nets with their shallow counterparts. Another possibility is to extend results involving Boolean functions to Euclidean spaces, perhaps using spectral methods.
\end{abstract}

\bibliographystyle{plain}
\bibliography{refs}

\end{document}
